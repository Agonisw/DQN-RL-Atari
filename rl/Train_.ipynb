{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DuelCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    CNN with Duel Algo. https://arxiv.org/abs/1511.06581\n",
        "    \"\"\"\n",
        "    def __init__(self, h, w, output_size):\n",
        "        super(DuelCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=4,  out_channels=32, kernel_size=8, stride=4)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        convw, convh = self.conv2d_size_calc(w, h, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        convw, convh = self.conv2d_size_calc(convw, convh, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        convw, convh = self.conv2d_size_calc(convw, convh, kernel_size=3, stride=1)\n",
        "\n",
        "        linear_input_size = convw * convh * 64  # Last conv layer's out sizes\n",
        "\n",
        "        self.Alinear1 = nn.Linear(in_features=linear_input_size, out_features=128)\n",
        "        self.Alrelu = nn.LeakyReLU()  # Linear 1 activation funct\n",
        "        self.Alinear2 = nn.Linear(in_features=128, out_features=output_size)\n",
        "\n",
        "        self.Vlinear1 = nn.Linear(in_features=linear_input_size, out_features=128)\n",
        "        self.Vlrelu = nn.LeakyReLU()  # Linear 1 activation funct\n",
        "        self.Vlinear2 = nn.Linear(in_features=128, out_features=1)  # Only 1 node\n",
        "\n",
        "    def conv2d_size_calc(self, w, h, kernel_size=5, stride=2):\n",
        "        \"\"\"\n",
        "        Calcs conv layers output image sizes\n",
        "        \"\"\"\n",
        "        next_w = (w - (kernel_size - 1) - 1) // stride + 1\n",
        "        next_h = (h - (kernel_size - 1) - 1) // stride + 1\n",
        "        return next_w, next_h\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "\n",
        "        x = x.view(x.size(0), -1)  # Flatten every batch\n",
        "\n",
        "        Ax = self.Alrelu(self.Alinear1(x))\n",
        "        Ax = self.Alinear2(Ax)  # No activation on last layer\n",
        "\n",
        "        Vx = self.Vlrelu(self.Vlinear1(x))\n",
        "        Vx = self.Vlinear2(Vx)  # No activation on last layer\n",
        "\n",
        "        q = Vx + (Ax - Ax.mean())\n",
        "\n",
        "        return q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gym\n",
        "import cv2\n",
        "\n",
        "import time\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "\n",
        "env_name = \"PongDeterministic-v4\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "save_models = True\n",
        "Train = True\n",
        "Load_model = False \n",
        "Load_file_ep = 900 \n",
        "Batch_size = 64\n",
        "mem_size = 40000\n",
        "Gamma = 0.97\n",
        "Alpha = 0.00025 \n",
        "Epsilon_decay = 0.99 \n",
        "Render = False  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class Agent:\n",
        "    def __init__(self, environment):\n",
        "        \"\"\"\n",
        "        Hyperparameters definition for Agent\n",
        "        \"\"\"\n",
        "        self.state_size_h = environment.observation_space.shape[0]\n",
        "        self.state_size_w = environment.observation_space.shape[1]\n",
        "        self.state_size_c = environment.observation_space.shape[2]\n",
        "\n",
        "        self.action_size = environment.action_space.n\n",
        "\n",
        "        self.target_h = 80  \n",
        "        self.target_w = 64  \n",
        "\n",
        "        self.crop_dim = [20, self.state_size_h, 0, self.state_size_w] \n",
        "\n",
        "        self.gamma = Gamma  \n",
        "        self.alpha = Alpha  \n",
        "\n",
        "        self.epsilon = 1  \n",
        "        self.epsilon_decay = Epsilon_decay \n",
        "        self.epsilon_minimum = 0.05 \n",
        "\n",
        "        self.memory = deque(maxlen=50000)\n",
        "\n",
        "        # Create two model for DDQN algorithm\n",
        "        self.online_model = DuelCNN(h=self.target_h, w=self.target_w, output_size=self.action_size).to(device)\n",
        "        self.target_model = DuelCNN(h=self.target_h, w=self.target_w, output_size=self.action_size).to(device)\n",
        "        self.target_model.load_state_dict(self.online_model.state_dict())\n",
        "        self.target_model.eval()\n",
        "\n",
        "        self.optimizer = optim.Adam(self.online_model.parameters(), lr=self.alpha)\n",
        "\n",
        "    def process_image(self, image):\n",
        "        frame = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # To grayscale\n",
        "        frame = frame[self.crop_dim[0]:self.crop_dim[1], self.crop_dim[2]:self.crop_dim[3]]  # Cut 20 px from top\n",
        "        frame = cv2.resize(frame, (self.target_w, self.target_h))  # Resize\n",
        "        frame = frame.reshape(self.target_w, self.target_h) / 255  # Normalize\n",
        "\n",
        "        return frame\n",
        "\n",
        "    def select_action(self, state):\n",
        "        act_protocol = 'Explore' if random.uniform(0, 1) <= self.epsilon else 'Exploit'\n",
        "        if act_protocol == 'Explore':\n",
        "            action = random.randrange(self.action_size)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                state = torch.tensor(state, dtype=torch.float, device=device).unsqueeze(0)\n",
        "                q_values = self.online_model.forward(state)  # (1, action_size)\n",
        "                action = torch.argmax(q_values).item()  # Returns the indices of the maximum value of all elements\n",
        "        return action\n",
        "\n",
        "    def train(self):\n",
        "        if len(agent.memory) < mem_size:\n",
        "            loss, max_q = [0, 0]\n",
        "            return loss, max_q\n",
        "        state, action, reward, next_state, done = zip(*random.sample(self.memory, Batch_size))\n",
        "\n",
        "        state = np.concatenate(state)\n",
        "        next_state = np.concatenate(next_state)\n",
        "\n",
        "        # Convert them to tensors\n",
        "        state = torch.tensor(state, dtype=torch.float, device=device)\n",
        "        next_state = torch.tensor(next_state, dtype=torch.float, device=device)\n",
        "        action = torch.tensor(action, dtype=torch.long, device=device)\n",
        "        reward = torch.tensor(reward, dtype=torch.float, device=device)\n",
        "        done = torch.tensor(done, dtype=torch.float, device=device)\n",
        "\n",
        "        # Make predictions\n",
        "        state_q_values = self.online_model(state)\n",
        "        next_states_q_values = self.online_model(next_state)\n",
        "        next_states_target_q_values = self.target_model(next_state)\n",
        "\n",
        "        selected_q_value = state_q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "        next_states_target_q_value = next_states_target_q_values.gather(1, next_states_q_values.max(1)[1].unsqueeze(1)).squeeze(1)\n",
        "        # Use Bellman function to find expected q value\n",
        "        expected_q_value = reward + self.gamma * next_states_target_q_value * (1 - done)\n",
        "\n",
        "        # Calc loss with expected_q_value and q_value\n",
        "        loss = (selected_q_value - expected_q_value.detach()).pow(2).mean()\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss, torch.max(state_q_values).item()\n",
        "\n",
        "    def storeResults(self, state, action, reward, nextState, done):\n",
        "        \"\"\"\n",
        "        Store every result to memory\n",
        "        \"\"\"\n",
        "        self.memory.append([state[None, :], action, reward, nextState[None, :], done])\n",
        "\n",
        "    def adaptiveEpsilon(self):\n",
        "        if self.epsilon > self.epsilon_minimum:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    environment = gym.make(env_name)  # Get env\n",
        "    agent = Agent(environment)  # Create Agent\n",
        "\n",
        "    if Load_model:\n",
        "        agent.online_model.load_state_dict(torch.load(\"./models/pong-\"+str(Load_file_ep)+\".pkl\"))\n",
        "\n",
        "        with open(\"./models/pong-\"+str(Load_file_ep)+'.json') as outfile:\n",
        "            param = json.load(outfile)\n",
        "            agent.epsilon = param.get('epsilon')\n",
        "\n",
        "        startEpisode = Load_file_ep + 1\n",
        "\n",
        "    else:\n",
        "        startEpisode = 1\n",
        "\n",
        "    last_100_ep_reward = deque(maxlen=100) \n",
        "    total_step = 1  \n",
        "    for episode in range(startEpisode, 100000):\n",
        "\n",
        "        startTime = time.time()  # Keep time\n",
        "        state = environment.reset()  # Reset env\n",
        "\n",
        "        state = agent.process_image(state)  # Process image\n",
        "\n",
        "        state = np.stack((state, state, state, state))\n",
        "\n",
        "        total_max_q_val = 0  # Total max q vals\n",
        "        total_reward = 0  # Total reward for each episode\n",
        "        total_loss = 0  # Total loss for each episode\n",
        "        for step in range(100000):\n",
        "\n",
        "            if Render:\n",
        "                environment.render()  # Show state visually\n",
        "\n",
        "            action = agent.select_action(state)  # Act\n",
        "            next_state, reward, done, info = environment.step(action)  # Observe\n",
        "\n",
        "            next_state = agent.process_image(next_state)  # Process image\n",
        "\n",
        "            next_state = np.stack((next_state, state[0], state[1], state[2]))\n",
        "\n",
        "            agent.storeResults(state, action, reward, next_state, done)  # Store to mem\n",
        "\n",
        "            state = next_state \n",
        "\n",
        "            if Train:\n",
        "             \n",
        "                loss, max_q_val = agent.train()  \n",
        "            else:\n",
        "                loss, max_q_val = [0, 0]\n",
        "\n",
        "            total_loss += loss\n",
        "            total_max_q_val += max_q_val\n",
        "            total_reward += reward\n",
        "            total_step += 1\n",
        "            if total_step % 1000 == 0:\n",
        "                agent.adaptiveEpsilon()  # Decrase epsilon\n",
        "\n",
        "            if done:  # Episode completed\n",
        "                currentTime = time.time()  # Keep current time\n",
        "                time_passed = currentTime - startTime  # Find episode duration\n",
        "                current_time_format = time.strftime(\"%H:%M:%S\", time.gmtime())  # Get current dateTime as HH:MM:SS\n",
        "                epsilonDict = {'epsilon': agent.epsilon}  # Create epsilon dict to save model as file\n",
        "\n",
        "                if save_models and episode % 10 == 0:  # Save model as file\n",
        "                    weightsPath = \"./models/pong-\" + str(episode) + '.pkl'\n",
        "                    epsilonPath = \"./models/pong-\" + str(episode) + '.json'\n",
        "\n",
        "                    torch.save(agent.online_model.state_dict(), weightsPath)\n",
        "                    with open(epsilonPath, 'w') as outfile:\n",
        "                        json.dump(epsilonDict, outfile)\n",
        "\n",
        "                if Train:\n",
        "                    agent.target_model.load_state_dict(agent.online_model.state_dict())\n",
        "\n",
        "                last_100_ep_reward.append(total_reward)\n",
        "                avg_max_q_val = total_max_q_val / step\n",
        "\n",
        "                outStr = \"Episode:{} Time:{} Reward:{:.2f} Loss:{:.2f} Last_100_Avg_Rew:{:.3f} Avg_Max_Q:{:.3f} Epsilon:{:.2f} Duration:{:.2f} Step:{} CStep:{}\".format(\n",
        "                    episode, current_time_format, total_reward, total_loss, np.mean(last_100_ep_reward), avg_max_q_val, agent.epsilon, time_passed, step, total_step\n",
        "                )\n",
        "\n",
        "                print(outStr)\n",
        "\n",
        "                if save_models:\n",
        "                    outputPath = \"./models/pong-\" + \"out\" + '.txt'  # Save outStr to file\n",
        "                    with open(outputPath, 'a') as outfile:\n",
        "                        outfile.write(outStr+\"\\n\")\n",
        "\n",
        "                break\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "OpenAIPong-DQN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.9 ('dqn')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "2a22c212fd8aa3d0b90120d3e7e4c88582a4902addc93710c2e58244a2f8beb3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
